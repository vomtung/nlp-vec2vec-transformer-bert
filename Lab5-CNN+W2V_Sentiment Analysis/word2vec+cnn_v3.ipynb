{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22959,
     "status": "ok",
     "timestamp": 1653989276555,
     "user": {
      "displayName": "Đức Nguyễn Quang",
      "userId": "15255943122151670013"
     },
     "user_tz": -420
    },
    "id": "sQic7KJRaSP6",
    "outputId": "ecec867d-34a0-4c95-d5d8-d51e999e0ff3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4639,
     "status": "ok",
     "timestamp": 1653989252196,
     "user": {
      "displayName": "Đức Nguyễn Quang",
      "userId": "15255943122151670013"
     },
     "user_tz": -420
    },
    "id": "aoVjFKmOZMZ8",
    "outputId": "bcd119ff-e091-4cf5-ca96-45e131211e1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyvi\n",
      "  Downloading pyvi-0.1.1-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: scikit-learn in e:\\software\\python\\lib\\site-packages (from pyvi) (1.5.2)\n",
      "Collecting sklearn-crfsuite (from pyvi)\n",
      "  Downloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in e:\\software\\python\\lib\\site-packages (from scikit-learn->pyvi) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in e:\\software\\python\\lib\\site-packages (from scikit-learn->pyvi) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in e:\\software\\python\\lib\\site-packages (from scikit-learn->pyvi) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in e:\\software\\python\\lib\\site-packages (from scikit-learn->pyvi) (3.5.0)\n",
      "Collecting python-crfsuite>=0.9.7 (from sklearn-crfsuite->pyvi)\n",
      "  Downloading python_crfsuite-0.9.11-cp312-cp312-win_amd64.whl.metadata (4.4 kB)\n",
      "Collecting tabulate>=0.4.2 (from sklearn-crfsuite->pyvi)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: tqdm>=2.0 in e:\\software\\python\\lib\\site-packages (from sklearn-crfsuite->pyvi) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\vomtu\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=2.0->sklearn-crfsuite->pyvi) (0.4.6)\n",
      "Downloading pyvi-0.1.1-py2.py3-none-any.whl (8.5 MB)\n",
      "   ---------------------------------------- 0.0/8.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/8.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/8.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/8.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/8.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/8.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/8.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/8.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/8.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/8.5 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.5/8.5 MB 294.2 kB/s eta 0:00:27\n",
      "   -- ------------------------------------- 0.5/8.5 MB 294.2 kB/s eta 0:00:27\n",
      "   -- ------------------------------------- 0.5/8.5 MB 294.2 kB/s eta 0:00:27\n",
      "   -- ------------------------------------- 0.5/8.5 MB 294.2 kB/s eta 0:00:27\n",
      "   --- ------------------------------------ 0.8/8.5 MB 302.2 kB/s eta 0:00:26\n",
      "   --- ------------------------------------ 0.8/8.5 MB 302.2 kB/s eta 0:00:26\n",
      "   --- ------------------------------------ 0.8/8.5 MB 302.2 kB/s eta 0:00:26\n",
      "   --- ------------------------------------ 0.8/8.5 MB 302.2 kB/s eta 0:00:26\n",
      "   ---- ----------------------------------- 1.0/8.5 MB 310.6 kB/s eta 0:00:24\n",
      "   ---- ----------------------------------- 1.0/8.5 MB 310.6 kB/s eta 0:00:24\n",
      "   ---- ----------------------------------- 1.0/8.5 MB 310.6 kB/s eta 0:00:24\n",
      "   ---- ----------------------------------- 1.0/8.5 MB 310.6 kB/s eta 0:00:24\n",
      "   ------ --------------------------------- 1.3/8.5 MB 321.1 kB/s eta 0:00:23\n",
      "   ------ --------------------------------- 1.3/8.5 MB 321.1 kB/s eta 0:00:23\n",
      "   ------ --------------------------------- 1.3/8.5 MB 321.1 kB/s eta 0:00:23\n",
      "   ------- -------------------------------- 1.6/8.5 MB 325.1 kB/s eta 0:00:22\n",
      "   ------- -------------------------------- 1.6/8.5 MB 325.1 kB/s eta 0:00:22\n",
      "   ------- -------------------------------- 1.6/8.5 MB 325.1 kB/s eta 0:00:22\n",
      "   ------- -------------------------------- 1.6/8.5 MB 325.1 kB/s eta 0:00:22\n",
      "   -------- ------------------------------- 1.8/8.5 MB 333.3 kB/s eta 0:00:20\n",
      "   -------- ------------------------------- 1.8/8.5 MB 333.3 kB/s eta 0:00:20\n",
      "   -------- ------------------------------- 1.8/8.5 MB 333.3 kB/s eta 0:00:20\n",
      "   --------- ------------------------------ 2.1/8.5 MB 340.4 kB/s eta 0:00:19\n",
      "   --------- ------------------------------ 2.1/8.5 MB 340.4 kB/s eta 0:00:19\n",
      "   --------- ------------------------------ 2.1/8.5 MB 340.4 kB/s eta 0:00:19\n",
      "   ----------- ---------------------------- 2.4/8.5 MB 347.7 kB/s eta 0:00:18\n",
      "   ----------- ---------------------------- 2.4/8.5 MB 347.7 kB/s eta 0:00:18\n",
      "   ----------- ---------------------------- 2.4/8.5 MB 347.7 kB/s eta 0:00:18\n",
      "   ------------ --------------------------- 2.6/8.5 MB 354.4 kB/s eta 0:00:17\n",
      "   ------------ --------------------------- 2.6/8.5 MB 354.4 kB/s eta 0:00:17\n",
      "   ------------- -------------------------- 2.9/8.5 MB 363.1 kB/s eta 0:00:16\n",
      "   ------------- -------------------------- 2.9/8.5 MB 363.1 kB/s eta 0:00:16\n",
      "   ------------- -------------------------- 2.9/8.5 MB 363.1 kB/s eta 0:00:16\n",
      "   -------------- ------------------------- 3.1/8.5 MB 371.3 kB/s eta 0:00:15\n",
      "   -------------- ------------------------- 3.1/8.5 MB 371.3 kB/s eta 0:00:15\n",
      "   ---------------- ----------------------- 3.4/8.5 MB 379.8 kB/s eta 0:00:14\n",
      "   ---------------- ----------------------- 3.4/8.5 MB 379.8 kB/s eta 0:00:14\n",
      "   ----------------- ---------------------- 3.7/8.5 MB 391.6 kB/s eta 0:00:13\n",
      "   ----------------- ---------------------- 3.7/8.5 MB 391.6 kB/s eta 0:00:13\n",
      "   ------------------ --------------------- 3.9/8.5 MB 400.1 kB/s eta 0:00:12\n",
      "   ------------------ --------------------- 3.9/8.5 MB 400.1 kB/s eta 0:00:12\n",
      "   ------------------- -------------------- 4.2/8.5 MB 411.2 kB/s eta 0:00:11\n",
      "   ------------------- -------------------- 4.2/8.5 MB 411.2 kB/s eta 0:00:11\n",
      "   --------------------- ------------------ 4.5/8.5 MB 418.8 kB/s eta 0:00:10\n",
      "   --------------------- ------------------ 4.5/8.5 MB 418.8 kB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 4.7/8.5 MB 427.6 kB/s eta 0:00:09\n",
      "   ---------------------- ----------------- 4.7/8.5 MB 427.6 kB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 5.0/8.5 MB 437.0 kB/s eta 0:00:08\n",
      "   ----------------------- ---------------- 5.0/8.5 MB 437.0 kB/s eta 0:00:08\n",
      "   ------------------------ --------------- 5.2/8.5 MB 446.4 kB/s eta 0:00:08\n",
      "   -------------------------- ------------- 5.5/8.5 MB 455.3 kB/s eta 0:00:07\n",
      "   -------------------------- ------------- 5.5/8.5 MB 455.3 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 5.8/8.5 MB 464.2 kB/s eta 0:00:06\n",
      "   --------------------------- ------------ 5.8/8.5 MB 464.2 kB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 6.0/8.5 MB 472.6 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 6.3/8.5 MB 481.7 kB/s eta 0:00:05\n",
      "   ----------------------------- ---------- 6.3/8.5 MB 481.7 kB/s eta 0:00:05\n",
      "   ------------------------------- -------- 6.6/8.5 MB 489.8 kB/s eta 0:00:04\n",
      "   -------------------------------- ------- 6.8/8.5 MB 498.7 kB/s eta 0:00:04\n",
      "   -------------------------------- ------- 6.8/8.5 MB 498.7 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 7.1/8.5 MB 506.0 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 7.3/8.5 MB 512.4 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 7.3/8.5 MB 512.4 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 7.6/8.5 MB 519.6 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 7.9/8.5 MB 527.1 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 7.9/8.5 MB 527.1 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 8.1/8.5 MB 533.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------  8.4/8.5 MB 538.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.5/8.5 MB 537.8 kB/s eta 0:00:00\n",
      "Downloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading python_crfsuite-0.9.11-cp312-cp312-win_amd64.whl (301 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: tabulate, python-crfsuite, sklearn-crfsuite, pyvi\n",
      "Successfully installed python-crfsuite-0.9.11 pyvi-0.1.1 sklearn-crfsuite-0.5.0 tabulate-0.9.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pyvi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8TFX7wSAmg9y"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras.utils.np_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyvi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ViTokenizer\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mword2vec\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnp_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_categorical\n\u001b[0;32m      9\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras.utils.np_utils'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from string import digits\n",
    "from collections import Counter\n",
    "from pyvi import ViTokenizer\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from keras.utils.np_utils import to_categorical\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a7lMy03omg93",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(\"vlsp_sentiment_train.csv\", sep='\\t')\n",
    "data_train.columns =['Class', 'Data']\n",
    "data_test = pd.read_csv(\"vlsp_sentiment_test.csv\", sep='\\t')\n",
    "data_test.columns =['Class', 'Data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 944,
     "status": "ok",
     "timestamp": 1653989301990,
     "user": {
      "displayName": "Đức Nguyễn Quang",
      "userId": "15255943122151670013"
     },
     "user_tz": -420
    },
    "id": "4HR1jAzImg94",
    "outputId": "8e324c8c-0dbb-4a69-aeff-b03e40766c7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5100, 2)\n",
      "(1050, 2)\n"
     ]
    }
   ],
   "source": [
    "print(data_train.shape)\n",
    "print(data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jvrbwPfZmg95"
   },
   "outputs": [],
   "source": [
    "labels = data_train.iloc[:, 0].values\n",
    "reviews = data_train.iloc[:, 1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3HlbVeHimg95"
   },
   "outputs": [],
   "source": [
    "encoded_labels = []\n",
    "\n",
    "for label in labels:\n",
    "    if label == -1:\n",
    "        encoded_labels.append([1,0,0])\n",
    "    elif label == 0:\n",
    "        encoded_labels.append([0,1,0])\n",
    "    else:\n",
    "        encoded_labels.append([0,0,1])\n",
    "\n",
    "encoded_labels = np.array(encoded_labels)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lm4OCwxXmg96"
   },
   "outputs": [],
   "source": [
    "reviews_processed = []\n",
    "unlabeled_processed = [] \n",
    "for review in reviews:\n",
    "    review_cool_one = ''.join([char for char in review if char not in digits])\n",
    "    reviews_processed.append(review_cool_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nW2OZgkgmg97"
   },
   "outputs": [],
   "source": [
    "#Use PyVi for Vietnamese word tokenizer\n",
    "word_reviews = []\n",
    "all_words = []\n",
    "for review in reviews_processed:\n",
    "    review = ViTokenizer.tokenize(review.lower())\n",
    "    word_reviews.append(review.split())\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pTb0MeDRmg98"
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 400 # how big is each word vector\n",
    "MAX_VOCAB_SIZE = 10000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "MAX_SEQUENCE_LENGTH = 300 # max number of words in a comment to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jW-7mKtWmg9-"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-BHpPSLTmg9_"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(word_reviews)\n",
    "sequences_train = tokenizer.texts_to_sequences(word_reviews)\n",
    "word_index = tokenizer.word_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LlV3M2dimg9_"
   },
   "outputs": [],
   "source": [
    "data = pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = encoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1653989306259,
     "user": {
      "displayName": "Đức Nguyễn Quang",
      "userId": "15255943122151670013"
     },
     "user_tz": -420
    },
    "id": "4dl9VZ3Rmg-A",
    "outputId": "0d43e170-e903-4d5b-f3ec-18a8b911d072"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X train and X validation tensor: (5100, 300)\n",
      "Shape of label train and validation tensor: (5100, 3)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of X train and X validation tensor:',data.shape)\n",
    "print('Shape of label train and validation tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-KKSjJdJmg-A"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "word_vectors = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/Tài liệu HCMUT/Cách tiếp cận hiện đại trong xử lý ngôn ngữ tự nhiên/CNN+W2V_Sentiment Analysis/vi-model-CBOW.bin', binary=True)\n",
    "\n",
    "\n",
    "vocabulary_size=min(len(word_index)+1,MAX_VOCAB_SIZE)\n",
    "embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i>=MAX_VOCAB_SIZE:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = word_vectors[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n",
    "\n",
    "del(word_vectors)\n",
    "\n",
    "from keras.layers import Embedding\n",
    "embedding_layer = Embedding(vocabulary_size,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 381,
     "status": "ok",
     "timestamp": 1653989417080,
     "user": {
      "displayName": "Đức Nguyễn Quang",
      "userId": "15255943122151670013"
     },
     "user_tz": -420
    },
    "id": "njBANdn5mg-B",
    "outputId": "24647910-9144-4987-83a2-fe64e64a04ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 297, 100), dtype=tf.float32, name=None), name='conv1d_4/Relu:0', description=\"created by layer 'conv1d_4'\")\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 300)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 300, 400)     3167600     ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 298, 100)     120100      ['embedding[1][0]']              \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 297, 100)     160100      ['embedding[1][0]']              \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 296, 100)     200100      ['embedding[1][0]']              \n",
      "                                                                                                  \n",
      " max_pooling1d_3 (MaxPooling1D)  (None, 1, 100)      0           ['conv1d_3[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling1d_4 (MaxPooling1D)  (None, 1, 100)      0           ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling1d_5 (MaxPooling1D)  (None, 1, 100)      0           ['conv1d_5[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 3, 100)       0           ['max_pooling1d_3[0][0]',        \n",
      "                                                                  'max_pooling1d_4[0][0]',        \n",
      "                                                                  'max_pooling1d_5[0][0]']        \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 300)          0           ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 300)          0           ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 3)            903         ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,648,803\n",
      "Trainable params: 3,648,803\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, Dropout,concatenate\n",
    "from tensorflow.keras.layers import Reshape, Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import regularizers\n",
    "sequence_length = data.shape[1]\n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 100\n",
    "drop = 0.5\n",
    "\n",
    "inputs = Input(shape=(sequence_length,))\n",
    "embedding = embedding_layer(inputs)\n",
    "# reshape = Reshape((sequence_length,EMBEDDING_DIM,1))(embedding)\n",
    "\n",
    "conv_0 = Conv1D(num_filters, filter_sizes[0],activation='relu',kernel_regularizer=regularizers.l2(0.01))(embedding)\n",
    "conv_1 = Conv1D(num_filters, filter_sizes[1],activation='relu',kernel_regularizer=regularizers.l2(0.01))(embedding)\n",
    "conv_2 = Conv1D(num_filters, filter_sizes[2],activation='relu',kernel_regularizer=regularizers.l2(0.01))(embedding)\n",
    "print(conv_1)\n",
    "maxpool_0 = MaxPooling1D(sequence_length - filter_sizes[0] + 1, strides=1)(conv_0)\n",
    "maxpool_1 = MaxPooling1D(sequence_length - filter_sizes[1] + 1, strides=1)(conv_1)\n",
    "maxpool_2 = MaxPooling1D(sequence_length - filter_sizes[2] + 1, strides=1)(conv_2)\n",
    "\n",
    "merged_tensor = concatenate([maxpool_0, maxpool_1, maxpool_2], axis=1)\n",
    "flatten = Flatten()(merged_tensor)\n",
    "reshape = Reshape((3*num_filters,))(flatten)\n",
    "dropout = Dropout(drop)(flatten)\n",
    "output = Dense(units=3, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout)\n",
    "\n",
    "\n",
    "# this creates a model that includes\n",
    "model = Model(inputs, output)\n",
    "\n",
    "\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "#define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n",
    "callbacks_list = [early_stopping]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10375,
     "status": "ok",
     "timestamp": 1653989547863,
     "user": {
      "displayName": "Đức Nguyễn Quang",
      "userId": "15255943122151670013"
     },
     "user_tz": -420
    },
    "id": "Jn0dBlzjmg-D",
    "outputId": "be7de957-bf78-4fff-bc31-b5e586f705fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "16/16 [==============================] - 2s 130ms/step - loss: 0.4944 - accuracy: 0.9784 - val_loss: 2.4030 - val_accuracy: 0.1706\n",
      "Epoch 2/5\n",
      "16/16 [==============================] - 2s 127ms/step - loss: 0.4781 - accuracy: 0.9755 - val_loss: 2.7319 - val_accuracy: 0.0882\n",
      "Epoch 3/5\n",
      "16/16 [==============================] - 2s 128ms/step - loss: 0.4636 - accuracy: 0.9748 - val_loss: 2.2442 - val_accuracy: 0.1882\n",
      "Epoch 4/5\n",
      "16/16 [==============================] - 2s 127ms/step - loss: 0.4541 - accuracy: 0.9733 - val_loss: 2.2611 - val_accuracy: 0.1833\n",
      "Epoch 5/5\n",
      "16/16 [==============================] - 2s 127ms/step - loss: 0.4349 - accuracy: 0.9777 - val_loss: 2.4392 - val_accuracy: 0.1451\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1817995450>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(data, labels, validation_split=0.2,\n",
    "          epochs=5, batch_size=256, callbacks=callbacks_list, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8XoN2UOamg-D"
   },
   "outputs": [],
   "source": [
    "labels_test = data_test.iloc[:, 0].values\n",
    "reviews_test = data_test.iloc[:, 1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PwiYb3Ohmg-E"
   },
   "outputs": [],
   "source": [
    "encoded_labels_test = []\n",
    "\n",
    "for label_test in labels_test:\n",
    "    if label_test == -1:\n",
    "        encoded_labels_test.append([1,0,0])\n",
    "    elif label_test == 0:\n",
    "        encoded_labels_test.append([0,1,0])\n",
    "    else:\n",
    "        encoded_labels_test.append([0,0,1])\n",
    "\n",
    "encoded_labels_test = np.array(encoded_labels_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E08tBw9img-E"
   },
   "outputs": [],
   "source": [
    "reviews_processed_test = []\n",
    "unlabeled_processed_test = [] \n",
    "for review_test in reviews_test:\n",
    "    review_cool_one = ''.join([char for char in review_test if char not in digits])\n",
    "    reviews_processed_test.append(review_cool_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OwgI9Xywmg-E"
   },
   "outputs": [],
   "source": [
    "#Use PyVi for Vietnamese word tokenizer\n",
    "word_reviews_test = []\n",
    "all_words = []\n",
    "for review_test in reviews_processed_test:\n",
    "    review_test = ViTokenizer.tokenize(review_test.lower())\n",
    "    word_reviews_test.append(review_test.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p02GxCh6mg-F"
   },
   "outputs": [],
   "source": [
    "sequences_test = tokenizer.texts_to_sequences(word_reviews_test)\n",
    "data_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels_test = encoded_labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 534,
     "status": "ok",
     "timestamp": 1653989480441,
     "user": {
      "displayName": "Đức Nguyễn Quang",
      "userId": "15255943122151670013"
     },
     "user_tz": -420
    },
    "id": "jAqUMGInmg-F",
    "outputId": "40acf056-b9c9-42ed-c767-423cc2054383"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X train and X validation tensor: (1050, 300)\n",
      "Shape of label train and validation tensor: (1050, 3)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of X train and X validation tensor:',data_test.shape)\n",
    "print('Shape of label train and validation tensor:', labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 596,
     "status": "ok",
     "timestamp": 1653989548454,
     "user": {
      "displayName": "Đức Nguyễn Quang",
      "userId": "15255943122151670013"
     },
     "user_tz": -420
    },
    "id": "LKclttiOmg-F",
    "outputId": "6abe8e2e-7ed4-439f-8899-e6b30a044758"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 6ms/step - loss: 1.2146 - accuracy: 0.6505\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(data_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 355,
     "status": "ok",
     "timestamp": 1653989552805,
     "user": {
      "displayName": "Đức Nguyễn Quang",
      "userId": "15255943122151670013"
     },
     "user_tz": -420
    },
    "id": "r31_uxxgmg-G",
    "outputId": "2fb9d570-546b-4652-ca5f-5e6794fc6198"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 121.46%\n",
      "accuracy: 65.05%\n"
     ]
    }
   ],
   "source": [
    "print(\"%s: %.2f%%\" % (model.metrics_names[0], score[0]*100))\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v8O3z4IFmg-G"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "word2vec+cnn_v3.ipynb",
   "provenance": [
    {
     "file_id": "1rFZZf9ECknkLNDv8so_kQNPhqain0RqJ",
     "timestamp": 1653989613942
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
